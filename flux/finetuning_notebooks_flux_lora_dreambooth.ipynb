{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "ðŸ“Œ **This notebook has been updated in [jhj0517/finetuning-notebooks](https://github.com/jhj0517/finetuning-notebooks) repository!**\n",
        "\n",
        "## Version : 1.0.6\n",
        "---"
      ],
      "metadata": {
        "id": "doKhBBXIfS21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #(Optional) Check GPU\n",
        "\n",
        "#@markdown To train Flux LoRA, at least 32GB VRAM (A100 in Colab) is recommended.\n",
        "#@markdown <br>You can check your GPU setup before start.\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "23yZvUlagEsx",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNbSbsctxahq",
        "outputId": "6af7aa18-4976-44f1-f88a-ff965b177115",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'ai-toolkit' already exists and is not an empty directory.\n",
            "/content/ai-toolkit\n",
            "Collecting git+https://github.com/huggingface/diffusers@363d1ab7e24c5ed6c190abb00df66d9edb74383b (from -r requirements.txt (line 5))\n",
            "  Cloning https://github.com/huggingface/diffusers (to revision 363d1ab7e24c5ed6c190abb00df66d9edb74383b) to /tmp/pip-req-build-0eurb1zy\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/diffusers /tmp/pip-req-build-0eurb1zy\n",
            "  Running command git rev-parse -q --verify 'sha^363d1ab7e24c5ed6c190abb00df66d9edb74383b'\n",
            "  Running command git fetch -q https://github.com/huggingface/diffusers 363d1ab7e24c5ed6c190abb00df66d9edb74383b\n",
            "  Running command git checkout -q 363d1ab7e24c5ed6c190abb00df66d9edb74383b\n",
            "  Resolved https://github.com/huggingface/diffusers to commit 363d1ab7e24c5ed6c190abb00df66d9edb74383b\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (2.6.0)\n",
            "Requirement already satisfied: torchvision==0.21.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (0.21.0)\n",
            "Requirement already satisfied: torchao==0.9.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (0.9.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (0.6.2)\n",
            "Requirement already satisfied: transformers==4.49.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (4.49.0)\n",
            "Requirement already satisfied: lycoris-lora==1.8.3 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (1.8.3)\n",
            "Requirement already satisfied: flatten_json in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 8)) (0.1.14)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 9)) (6.0.2)\n",
            "Requirement already satisfied: oyaml in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 10)) (1.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 11)) (2.19.0)\n",
            "Requirement already satisfied: kornia in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 12)) (0.8.1)\n",
            "Requirement already satisfied: invisible-watermark in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 13)) (0.2.0)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 14)) (0.8.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 15)) (1.10.1)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 16)) (0.10.2)\n",
            "Requirement already satisfied: albumentations==1.4.15 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 17)) (1.4.15)\n",
            "Requirement already satisfied: albucore==0.0.16 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 18)) (0.0.16)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 19)) (2.11.7)\n",
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 20)) (2.3.0)\n",
            "Requirement already satisfied: k-diffusion in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 21)) (0.1.1.post1)\n",
            "Requirement already satisfied: open_clip_torch in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 22)) (3.1.0)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 23)) (1.0.19)\n",
            "Requirement already satisfied: prodigyopt in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 24)) (1.1.2)\n",
            "Requirement already satisfied: controlnet_aux==0.0.7 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 25)) (0.0.7)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 26)) (1.1.1)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 27)) (0.47.0)\n",
            "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 28)) (0.1.9)\n",
            "Requirement already satisfied: lpips in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 29)) (0.1.4)\n",
            "Requirement already satisfied: pytorch_fid in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 30)) (0.3.0)\n",
            "Requirement already satisfied: optimum-quanto==0.2.4 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 31)) (0.2.4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 32)) (0.2.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 33)) (0.34.4)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 34)) (0.17.1)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 35)) (5.44.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 36)) (8.0.4)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 37)) (4.11.0.86)\n",
            "Requirement already satisfied: pytorch-wavelets==1.3.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 38)) (1.3.0)\n",
            "Requirement already satisfied: numpy==1.26.3 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 39)) (1.26.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0->-r requirements.txt (line 1)) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0->-r requirements.txt (line 1)) (4.15.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0->-r requirements.txt (line 1)) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0->-r requirements.txt (line 1)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0->-r requirements.txt (line 1)) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0->-r requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0->-r requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0->-r requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0->-r requirements.txt (line 1)) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0->-r requirements.txt (line 1)) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0->-r requirements.txt (line 1)) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0->-r requirements.txt (line 1)) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0->-r requirements.txt (line 1)) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0->-r requirements.txt (line 1)) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0->-r requirements.txt (line 1)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0->-r requirements.txt (line 1)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0->-r requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0->-r requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0->-r requirements.txt (line 1)) (3.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0->-r requirements.txt (line 1)) (75.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0->-r requirements.txt (line 1)) (1.13.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision==0.21.0->-r requirements.txt (line 2)) (11.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.49.0->-r requirements.txt (line 6)) (25.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.49.0->-r requirements.txt (line 6)) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.49.0->-r requirements.txt (line 6)) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers==4.49.0->-r requirements.txt (line 6)) (0.21.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.49.0->-r requirements.txt (line 6)) (4.67.1)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from albumentations==1.4.15->-r requirements.txt (line 17)) (1.16.1)\n",
            "Requirement already satisfied: scikit-image>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from albumentations==1.4.15->-r requirements.txt (line 17)) (0.25.2)\n",
            "Requirement already satisfied: eval-type-backport in /usr/local/lib/python3.12/dist-packages (from albumentations==1.4.15->-r requirements.txt (line 17)) (0.2.2)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.12/dist-packages (from albumentations==1.4.15->-r requirements.txt (line 17)) (4.11.0.86)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.12/dist-packages (from controlnet_aux==0.0.7->-r requirements.txt (line 25)) (8.7.0)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.12/dist-packages (from optimum-quanto==0.2.4->-r requirements.txt (line 31)) (1.13.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from pytorch-wavelets==1.3.0->-r requirements.txt (line 38)) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy==1.13.1->torch==2.6.0->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 11)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 11)) (1.74.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 11)) (3.9)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 11)) (5.29.5)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 11)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 11)) (3.1.3)\n",
            "Requirement already satisfied: kornia_rs>=0.1.9 in /usr/local/lib/python3.12/dist-packages (from kornia->-r requirements.txt (line 12)) (0.1.9)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from invisible-watermark->-r requirements.txt (line 13)) (1.9.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate->-r requirements.txt (line 15)) (5.9.5)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->-r requirements.txt (line 19)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic->-r requirements.txt (line 19)) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->-r requirements.txt (line 19)) (0.4.1)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from omegaconf->-r requirements.txt (line 20)) (4.9.3)\n",
            "Requirement already satisfied: clean-fid in /usr/local/lib/python3.12/dist-packages (from k-diffusion->-r requirements.txt (line 21)) (0.1.35)\n",
            "Requirement already satisfied: clip-anytorch in /usr/local/lib/python3.12/dist-packages (from k-diffusion->-r requirements.txt (line 21)) (2.6.0)\n",
            "Requirement already satisfied: dctorch in /usr/local/lib/python3.12/dist-packages (from k-diffusion->-r requirements.txt (line 21)) (0.1.2)\n",
            "Requirement already satisfied: jsonmerge in /usr/local/lib/python3.12/dist-packages (from k-diffusion->-r requirements.txt (line 21)) (1.9.2)\n",
            "Requirement already satisfied: torchdiffeq in /usr/local/lib/python3.12/dist-packages (from k-diffusion->-r requirements.txt (line 21)) (0.2.5)\n",
            "Requirement already satisfied: torchsde in /usr/local/lib/python3.12/dist-packages (from k-diffusion->-r requirements.txt (line 21)) (0.2.6)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (from k-diffusion->-r requirements.txt (line 21)) (0.21.3)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.12/dist-packages (from open_clip_torch->-r requirements.txt (line 22)) (6.3.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->-r requirements.txt (line 33)) (1.1.9)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 35)) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 35)) (4.10.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 35)) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 35)) (0.116.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 35)) (0.6.1)\n",
            "Requirement already satisfied: gradio-client==1.12.1 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 35)) (1.12.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 35)) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 35)) (0.28.1)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 35)) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 35)) (3.11.3)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 35)) (2.2.2)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 35)) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 35)) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 35)) (0.12.12)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 35)) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 35)) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 35)) (0.47.3)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 35)) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 35)) (0.17.4)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 35)) (0.35.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.12.1->gradio->-r requirements.txt (line 35)) (15.0.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.12/dist-packages (from python-slugify->-r requirements.txt (line 36)) (1.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio->-r requirements.txt (line 35)) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio->-r requirements.txt (line 35)) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio->-r requirements.txt (line 35)) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio->-r requirements.txt (line 35)) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio->-r requirements.txt (line 35)) (0.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio->-r requirements.txt (line 35)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio->-r requirements.txt (line 35)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio->-r requirements.txt (line 35)) (2025.2)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.21.0->albumentations==1.4.15->-r requirements.txt (line 17)) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.21.0->albumentations==1.4.15->-r requirements.txt (line 17)) (2025.8.28)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.21.0->albumentations==1.4.15->-r requirements.txt (line 17)) (0.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio->-r requirements.txt (line 35)) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio->-r requirements.txt (line 35)) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio->-r requirements.txt (line 35)) (13.9.4)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy->open_clip_torch->-r requirements.txt (line 22)) (0.2.13)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata->controlnet_aux==0.0.7->-r requirements.txt (line 25)) (3.23.0)\n",
            "Requirement already satisfied: jsonschema>2.4.0 in /usr/local/lib/python3.12/dist-packages (from jsonmerge->k-diffusion->-r requirements.txt (line 21)) (4.25.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.49.0->-r requirements.txt (line 6)) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.49.0->-r requirements.txt (line 6)) (2.5.0)\n",
            "Requirement already satisfied: trampoline>=0.1.2 in /usr/local/lib/python3.12/dist-packages (from torchsde->k-diffusion->-r requirements.txt (line 21)) (0.1.2)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb->k-diffusion->-r requirements.txt (line 21)) (3.1.45)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb->k-diffusion->-r requirements.txt (line 21)) (4.4.0)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb->k-diffusion->-r requirements.txt (line 21)) (2.37.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->k-diffusion->-r requirements.txt (line 21)) (4.0.12)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>2.4.0->jsonmerge->k-diffusion->-r requirements.txt (line 21)) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>2.4.0->jsonmerge->k-diffusion->-r requirements.txt (line 21)) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>2.4.0->jsonmerge->k-diffusion->-r requirements.txt (line 21)) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>2.4.0->jsonmerge->k-diffusion->-r requirements.txt (line 21)) (0.27.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio->-r requirements.txt (line 35)) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio->-r requirements.txt (line 35)) (2.19.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->k-diffusion->-r requirements.txt (line 21)) (5.0.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio->-r requirements.txt (line 35)) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "#@title #1. Install Dependencies\n",
        "#@markdown This notebook is powered by https://github.com/ostris/ai-toolkit\n",
        "#@markdown <br>You can ignore the \"Restart session\" popup after you run the cell.\n",
        "!git clone -b fix/numpy-error https://github.com/jhj0517/ai-toolkit.git\n",
        "%cd ai-toolkit\n",
        "!git submodule update --init --recursive\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "# To fix numpy incompatibility from https://github.com/ostris/ai-toolkit/issues/267\n",
        "import os\n",
        "!pip install --quiet --force-reinstall --no-deps numpy==1.26.3\n",
        "os.kill(os.getpid(), 9)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # 2. (Optional) Mount Google Drive\n",
        "\n",
        "#@markdown It's not mandatory but it's recommended to mount to Google Drive and use the Google Drive's path for your training image dataset.\n",
        "\n",
        "#@markdown The dataset should have following structure:\n",
        "\n",
        "#@markdown Each image file should have a corresponding text file (`.txt`) with the same name.\n",
        "#@markdown The text file contains prompts associated with the image.\n",
        "\n",
        "#@markdown ### Example File Structure:\n",
        "#@markdown ```\n",
        "#@markdown your-dataset/\n",
        "#@markdown â”œâ”€â”€ a (1).png         # Image file\n",
        "#@markdown â”œâ”€â”€ a (1).txt         # Corresponding prompt for a (1).png\n",
        "#@markdown â”œâ”€â”€ a (2).png         # Another image file\n",
        "#@markdown â”œâ”€â”€ a (2).txt         # Corresponding prompt for a (2).png\n",
        "#@markdown ```\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "M1bu3MpsACOu",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # 3. (Optional) Register Huggingface Token To Download Base Model\n",
        "\n",
        "#@markdown If you don't have entire base model files ([black-forest-labs/FLUX.1-dev](https://huggingface.co/black-forest-labs/FLUX.1-dev)) in the drive you need to sign in to Huggingface to download the model.\n",
        "\n",
        "#@markdown Get your tokens from https://huggingface.co/settings/tokens, and register it in colab's seceret as **`HF_TOKEN`** and use it in any notebook. ( 'Read' permission is enough )\n",
        "\n",
        "#@markdown To register secrets in colab, click on the key-shaped icon in the left panel and enter your **`HF_TOKEN`** like this:\n",
        "\n",
        "#@markdown ![image](https://media.githubusercontent.com/media/jhj0517/finetuning-notebooks/master/docs/screenshots/colab_secrets.png)\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "os.environ['HF_TOKEN'] = hf_token\n",
        "\n",
        "print(\"HF_TOKEN environment variable has been set.\")"
      ],
      "metadata": {
        "id": "9WzQRwZij5jf",
        "outputId": "2930a178-9aab-4a14-b9cd-155b930a7ef8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SecretNotFoundError",
          "evalue": "Secret HF_TOKEN does not exist.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSecretNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1943843797.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mhf_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'HF_TOKEN'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'HF_TOKEN'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhf_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/userdata.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exists'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mSecretNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'access'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mNotebookAccessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSecretNotFoundError\u001b[0m: Secret HF_TOKEN does not exist."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # 4. Train with Parameters\n",
        "import os\n",
        "import sys\n",
        "sys.path.append('/content/ai-toolkit')\n",
        "from toolkit.job import run_job\n",
        "from collections import OrderedDict\n",
        "from PIL import Image\n",
        "import os\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
        "\n",
        "#@markdown ## Paths Configuration\n",
        "\n",
        "#@markdown Set your dataset path and output path for lora here.\n",
        "DATASET_DIR = \"/content/drive/MyDrive/aria/\" # @param {type:\"string\"}\n",
        "OUTPUT_DIR = '/content/drive/MyDrive/finetuning-notebooks/flux/outputs'  # @param {type:\"string\"}\n",
        "LORA_NAME = 'Aria'  # @param {type:\"string\"}\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "#@markdown ## Base Model Configuration\n",
        "#@markdown If you'll just use the default repo id here then you need to register huggingface token in the previous section\n",
        "repo_id_or_path = 'black-forest-labs/FLUX.1-dev' # @param {type:\"string\"}\n",
        "quantize = False # @param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ## Process Settings\n",
        "#@markdown (max_step_saves_to_keep = how many checkpoints to keep during training. )\n",
        "dtype = \"float16\" # @param {type:\"string\"}\n",
        "save_every = 250 # @param {type:\"number\"}\n",
        "max_step_saves_to_keep = 10 # @param {type:\"number\"}\n",
        "#@markdown Whenever `sample_every` step, it will make samples to the output directory with prompts below to benchmark your result.\n",
        "#@markdown <br>Below is the example with the trigger word \"A teddy dog\". The \"trigger word\" thing is not necessary.\n",
        "sample_every = 250 # @param {type:\"number\"}\n",
        "sample_seed = 77 # @param {type: \"number\"}\n",
        "sample_steps = 20 # @param {type: \"number\"}\n",
        "sample_prompt_1 = \"@Aria professional headshot, studio lighting, soft focus background, natural makeup, confident smile, photorealistic, detailed skin texture, 35mm lens, f/2.8\" # @param {type: \"string\"}\n",
        "sample_prompt_2 = \"@Aria full-body portrait standing in a modern apartment, wearing casual jeans and white t-shirt, natural window lighting, relaxed pose, hands in pockets, photorealistic, 50mm lens\" # @param {type: \"string\"}\n",
        "sample_prompt_3 = \"@Aria sitting at a cafe table, wearing a summer dress, golden hour lighting, city background blurred, holding coffee cup, warm natural smile, photorealistic, shallow depth of field, 85mm lens\" # @param {type: \"string\"}\n",
        "### Add `sample_prompts` as much as you need  ###\n",
        "sample_prompts = [sample_prompt_1, sample_prompt_2, sample_prompt_3]\n",
        "\n",
        "performance_log_every = 1000 # @param {type:\"number\"}\n",
        "#@markdown ### Network\n",
        "#@markdown You can train only specific layers, `only_if_contains` is enabled when `train_only_specific_layers` is True..\n",
        "linear = 16 # @param {type:\"number\"}\n",
        "linear_alpha = 16 # @param {type:\"number\"}\n",
        "## network_kwargs\n",
        "train_only_specific_layers = False # @param {type:\"boolean\"}\n",
        "only_if_contains = [\"transformer.single_transformer_blocks.7.proj_out\", \"transformer.single_transformer_blocks.20.proj_out\"] # @param {type: \"raw\"}\n",
        "\n",
        "#@markdown ## Dataset Settings\n",
        "caption_ext = \"txt\" # @param {type:\"string\"}\n",
        "caption_dropout_rate = 0.05 # @param {type:\"number\"}\n",
        "shuffle_tokens = False # @param {type:\"boolean\"}\n",
        "cache_latents_to_disk = True # @param {type:\"boolean\"}\n",
        "resolution = \"512, 768, 1024\" # @param {type:\"string\"}\n",
        "resolution = [int(res.strip()) for res in resolution.split(\",\")]\n",
        "\n",
        "#@markdown ## Training Settings\n",
        "batch_size = 1 # @param {type:\"number\"}\n",
        "# Recommended range is 500 ~ 4000\n",
        "steps = 4000 # @param {type:\"number\"}\n",
        "gradient_accumulation_steps = 1 # @param {type:\"number\"}\n",
        "train_dtype = \"bf16\" # @param {type:\"string\"}\n",
        "lr = 4e-4 # @param {type:\"number\"}\n",
        "train_unet = True # @param {type:\"boolean\"}\n",
        "train_text_encoder = False # @param {type:\"boolean\"}\n",
        "content_or_style = 'balanced' # @param [\"content\", \"style\", \"balanced\"]\n",
        "gradient_checkpointing = True # @param {type:\"boolean\"}\n",
        "noise_scheduler = 'flowmatch' # @param {type:\"string\"}\n",
        "optimizer = 'adamw8bit' # @param {type:\"string\"}\n",
        "# ema settings\n",
        "use_ema = True # @param {type:\"boolean\"}\n",
        "ema_decay = 0.99 # @param {type:\"number\"}\n",
        "\n",
        "# Training\n",
        "job_to_run = OrderedDict([\n",
        "    ('job', 'extension'),\n",
        "    ('config', OrderedDict([\n",
        "        # this name will be the folder and filename name\n",
        "        ('name', LORA_NAME),\n",
        "        ('process', [\n",
        "            OrderedDict([\n",
        "                ('type', 'sd_trainer'),\n",
        "                ('training_folder', OUTPUT_DIR),\n",
        "                ('performance_log_every', 1000),\n",
        "                ('device', 'cuda:0'),\n",
        "                ('network', OrderedDict([\n",
        "                    ('type', 'lora'),\n",
        "                    ('linear', linear),\n",
        "                    ('linear_alpha', linear_alpha)\n",
        "                ])),\n",
        "                ('save', OrderedDict([\n",
        "                    ('dtype', dtype),\n",
        "                    ('save_every', save_every),\n",
        "                    ('max_step_saves_to_keep', max_step_saves_to_keep)\n",
        "                ])),\n",
        "                ('datasets', [\n",
        "                    OrderedDict([\n",
        "                        ('folder_path', DATASET_DIR),\n",
        "                        ('caption_ext', caption_ext),\n",
        "                        ('caption_dropout_rate', caption_dropout_rate),\n",
        "                        ('shuffle_tokens', shuffle_tokens),\n",
        "                        ('cache_latents_to_disk', cache_latents_to_disk),\n",
        "                        ('resolution', resolution)\n",
        "                    ])\n",
        "                ]),\n",
        "                ('train', OrderedDict([\n",
        "                    ('batch_size', batch_size),\n",
        "                    ('steps', steps),\n",
        "                    ('gradient_accumulation_steps', gradient_accumulation_steps),\n",
        "                    ('train_unet', train_unet),\n",
        "                    ('train_text_encoder', train_text_encoder),\n",
        "                    ('content_or_style', content_or_style),\n",
        "                    ('gradient_checkpointing', gradient_checkpointing),\n",
        "                    ('noise_scheduler', noise_scheduler),\n",
        "                    ('optimizer', optimizer),\n",
        "                    ('lr', lr),\n",
        "                    ('ema_config', OrderedDict([\n",
        "                        ('use_ema', use_ema),\n",
        "                        ('ema_decay', ema_decay)\n",
        "                    ])),\n",
        "                    ('dtype', train_dtype)\n",
        "                ])),\n",
        "                ('model', OrderedDict([\n",
        "                    ('name_or_path', repo_id_or_path),\n",
        "                    ('is_flux', True),\n",
        "                    ('quantize', quantize),\n",
        "                ])),\n",
        "                ('sample', OrderedDict([\n",
        "                    ('sampler', 'flowmatch'),\n",
        "                    ('sample_every', sample_every),\n",
        "                    ('width', 1024),\n",
        "                    ('height', 1024),\n",
        "                    ('prompts', sample_prompts),\n",
        "                    ('neg', ''),\n",
        "                    ('seed', sample_seed),\n",
        "                    ('walk_seed', True),\n",
        "                    ('guidance_scale', 4),\n",
        "                    ('sample_steps', sample_steps)\n",
        "                ]))\n",
        "            ])\n",
        "        ])\n",
        "    ])),\n",
        "    ('meta', OrderedDict([\n",
        "        ('name', '[name]'),\n",
        "        ('version', '1.0')\n",
        "    ]))\n",
        "])\n",
        "\n",
        "# Conditional Parameters\n",
        "if train_only_specific_layers:\n",
        "    network = job_to_run[\"config\"][\"process\"][0][\"network\"]\n",
        "    network_kwargs = network.setdefault(\"network_kwargs\", OrderedDict())\n",
        "    network_kwargs[\"only_if_contains\"] = only_if_contains\n",
        "\n",
        "run_job(job_to_run)\n"
      ],
      "metadata": {
        "id": "fob2cRMQeW5C",
        "outputId": "0bde4537-5676-49eb-9a09-ed64e248b440",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"type\": \"sd_trainer\",\n",
            "    \"training_folder\": \"/content/drive/MyDrive/finetuning-notebooks/flux/outputs\",\n",
            "    \"performance_log_every\": 1000,\n",
            "    \"device\": \"cuda:0\",\n",
            "    \"network\": {\n",
            "        \"type\": \"lora\",\n",
            "        \"linear\": 16,\n",
            "        \"linear_alpha\": 16\n",
            "    },\n",
            "    \"save\": {\n",
            "        \"dtype\": \"float16\",\n",
            "        \"save_every\": 250,\n",
            "        \"max_step_saves_to_keep\": 10\n",
            "    },\n",
            "    \"datasets\": [\n",
            "        {\n",
            "            \"folder_path\": \"/content/drive/MyDrive/aria/\",\n",
            "            \"caption_ext\": \"txt\",\n",
            "            \"caption_dropout_rate\": 0.05,\n",
            "            \"shuffle_tokens\": false,\n",
            "            \"cache_latents_to_disk\": true,\n",
            "            \"resolution\": [\n",
            "                512,\n",
            "                768,\n",
            "                1024\n",
            "            ]\n",
            "        }\n",
            "    ],\n",
            "    \"train\": {\n",
            "        \"batch_size\": 1,\n",
            "        \"steps\": 4000,\n",
            "        \"gradient_accumulation_steps\": 1,\n",
            "        \"train_unet\": true,\n",
            "        \"train_text_encoder\": false,\n",
            "        \"content_or_style\": \"balanced\",\n",
            "        \"gradient_checkpointing\": true,\n",
            "        \"noise_scheduler\": \"flowmatch\",\n",
            "        \"optimizer\": \"adamw8bit\",\n",
            "        \"lr\": 0.0004,\n",
            "        \"ema_config\": {\n",
            "            \"use_ema\": true,\n",
            "            \"ema_decay\": 0.99\n",
            "        },\n",
            "        \"dtype\": \"bf16\"\n",
            "    },\n",
            "    \"model\": {\n",
            "        \"name_or_path\": \"black-forest-labs/FLUX.1-dev\",\n",
            "        \"is_flux\": true,\n",
            "        \"quantize\": false\n",
            "    },\n",
            "    \"sample\": {\n",
            "        \"sampler\": \"flowmatch\",\n",
            "        \"sample_every\": 250,\n",
            "        \"width\": 1024,\n",
            "        \"height\": 1024,\n",
            "        \"prompts\": [\n",
            "            \"@Aria professional headshot, studio lighting, soft focus background, natural makeup, confident smile, photorealistic, detailed skin texture, 35mm lens, f/2.8\",\n",
            "            \"@Aria full-body portrait standing in a modern apartment, wearing casual jeans and white t-shirt, natural window lighting, relaxed pose, hands in pockets, photorealistic, 50mm lens\",\n",
            "            \"@Aria sitting at a cafe table, wearing a summer dress, golden hour lighting, city background blurred, holding coffee cup, warm natural smile, photorealistic, shallow depth of field, 85mm lens\"\n",
            "        ],\n",
            "        \"neg\": \"\",\n",
            "        \"seed\": 77,\n",
            "        \"walk_seed\": true,\n",
            "        \"guidance_scale\": 4,\n",
            "        \"sample_steps\": 20\n",
            "    }\n",
            "}\n",
            "Using EMA\n",
            "\n",
            "#############################################\n",
            "# Running job: Aria\n",
            "#############################################\n",
            "\n",
            "\n",
            "Running  1 process\n",
            "Loading Flux model\n",
            "Loading transformer\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "black-forest-labs/FLUX.1-dev is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `token` or log in with `huggingface-cli login`.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/black-forest-labs/FLUX.1-dev/resolve/main/transformer/config.json",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mGatedRepoError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/diffusers/configuration_utils.py\u001b[0m in \u001b[0;36mload_config\u001b[0;34m(cls, pretrained_model_name_or_path, return_unused_kwargs, return_commit_hash, **kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m                 \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m                 config_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    388\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1010\u001b[0;31m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[1;32m   1011\u001b[0m             \u001b[0;31m# Destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1116\u001b[0m         \u001b[0;31m# Otherwise, raise appropriate error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m         \u001b[0m_raise_on_head_call_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_call_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1657\u001b[0m         \u001b[0;31m# Unauthorized => likely a token issue => let's raise the actual error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1658\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mhead_call_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1659\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1545\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1546\u001b[0;31m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[1;32m   1547\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers, endpoint)\u001b[0m\n\u001b[1;32m   1462\u001b[0m     \u001b[0;31m# Retrieve metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1463\u001b[0;31m     r = _request_wrapper(\n\u001b[0m\u001b[1;32m   1464\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfollow_relative_redirects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         response = _request_wrapper(\n\u001b[0m\u001b[1;32m    287\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttp_backoff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry_on_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry_on_status_codes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m429\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    425\u001b[0m             )\n\u001b[0;32m--> 426\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGatedRepoError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mGatedRepoError\u001b[0m: 401 Client Error. (Request ID: Root=1-68cf2ac5-14286e153cda90e870c6d8fd;9f8233b8-b322-4b3c-9d74-51702b52f021)\n\nCannot access gated repo for url https://huggingface.co/black-forest-labs/FLUX.1-dev/resolve/main/transformer/config.json.\nAccess to model black-forest-labs/FLUX.1-dev is restricted. You must have access to it and be authenticated to access it. Please log in.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2904385786.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0mnetwork_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"only_if_contains\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monly_if_contains\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m \u001b[0mrun_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_to_run\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/ai-toolkit/toolkit/job.py\u001b[0m in \u001b[0;36mrun_job\u001b[0;34m(config, name)\u001b[0m\n\u001b[1;32m     41\u001b[0m ):\n\u001b[1;32m     42\u001b[0m     \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/ai-toolkit/jobs/ExtensionJob.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocess\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/ai-toolkit/jobs/process/BaseSDTrainProcess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1455\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhook_after_sd_init_before_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m         \u001b[0;31m# run base sd process run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1457\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1459\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_after_sample_image_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_step_hook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/ai-toolkit/toolkit/stable_diffusion_model.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    611\u001b[0m                     \u001b[0mbase_model_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 613\u001b[0;31m             transformer = FluxTransformer2DModel.from_pretrained(\n\u001b[0m\u001b[1;32m    614\u001b[0m                 \u001b[0mtransformer_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m                 \u001b[0msubfolder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfolder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/diffusers/models/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    970\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m         \u001b[0;31m# load config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m         config, unused_kwargs, commit_hash = cls.load_config(\n\u001b[0m\u001b[1;32m    973\u001b[0m             \u001b[0mconfig_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m             \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/diffusers/configuration_utils.py\u001b[0m in \u001b[0;36mload_config\u001b[0;34m(cls, pretrained_model_name_or_path, return_unused_kwargs, return_commit_hash, **kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m                 )\n\u001b[1;32m    401\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mRepositoryNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m                 raise EnvironmentError(\n\u001b[0m\u001b[1;32m    403\u001b[0m                     \u001b[0;34mf\"{pretrained_model_name_or_path} is not a local folder and is not a valid model identifier\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m                     \u001b[0;34m\" listed on 'https://huggingface.co/models'\\nIf this is a private repository, make sure to pass a\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: black-forest-labs/FLUX.1-dev is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `token` or log in with `huggingface-cli login`."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # 5. (Optional) Test Your LoRA\n",
        "import torch\n",
        "from diffusers import FluxPipeline\n",
        "from numba import cuda\n",
        "import os\n",
        "\n",
        "BASE_MODEL_REPO_ID_OR_PATH = \"black-forest-labs/FLUX.1-dev\" # @param {type: \"string\"}\n",
        "PROMPT = \"a Teddy dog looks up and says hello world\" # @param {type: \"string\"}\n",
        "YOUR_LORA_PATH = \"/content/drive/MyDrive/finetuning-notebooks/flux/outputs/something/Your-First-Lora-V1.safetensors\" # @param {type:\"string\"}\n",
        "\n",
        "pipe = FluxPipeline.from_pretrained(BASE_MODEL_REPO_ID_OR_PATH, torch_dtype=torch.bfloat16)\n",
        "pipe.to(\"cuda\")\n",
        "pipe.load_lora_weights(YOUR_LORA_PATH)\n",
        "\n",
        "out = pipe(\n",
        "    prompt=PROMPT,\n",
        "    guidance_scale=0.,\n",
        "    height=1024,\n",
        "    width=1024,\n",
        "    num_inference_steps=4,\n",
        "    max_sequence_length=256,\n",
        ").images[0]\n",
        "out.save(\"image.png\")\n",
        "\n",
        "from IPython.display import display\n",
        "display(out)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "TBvS17rJSrS2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}